# [Awesome-Autonomous-Driving-Tools-and-Theory](#Awesome-Autonomous-Driving-Tools-and-Theory)

This repository is a knowledge base for learning Machine Learning and related tools. It contains theoretical books, practical code examples, and useful utilities, suitable for both study and research in Reinforcement Learning and Deep Learning.

## üóÇÔ∏è Directory Structure Overview

- [Awesome-Autonomous-Driving-Tools-and-Theory](#Awesome-Autonomous-Driving-Tools-and-Theory)
  - [Mathematical Foundations](./Mathematical-Foundations/)
    - [[Note] Information Theory](./Mathematical-Foundations/Information-Theory.pdf)
    - [[Note] Statistics](./Mathematical-Foundations/Statistics.pdf)
  - [Reinforcement Learning](./Reinforcement-Learning/)
    - [[Book] Mathematical Foundations of Reinforcement Learning](./Reinforcement-Learning/Mathematical_Foundations_of_Reinforcement_Learning.pdf)
    - [[Code] Math of RL Book Code](./Reinforcement-Learning/Math-RL-Book-Code)
      - [README.md](./Reinforcement-Learning/Math-RL-Book-Code/README.md) 
  - [ROS](./ROS/)
    - [README.md](./ROS/README.md)
    - [[Code] Simple ROS Demo](./ROS/Simple-ROS-Demo/)
  - [Pytorch](./Pytorch/)
    - [[Code] Neural Network Model](./Pytorch/Neural-Network-Model)
    - [[Code] Embedding](./Pytorch/Embedding)
    - [[Code] RNN](./Pytorch/RNN)
    - [[Code] LSTM](./Pytorch/LSTM)
    - [[Code] How to use cuda](./Pytorch/train_with_cuda.py)
  - [LLM](./LLM/)
    - [Transformer](./LLM/Transformer/)
      - [Key Structure Image](./LLM/Transformer/img/)
      - [[Code] Transformer](./LLM/Transformer/Transformer/)
      - [[Paper] Attention Is All You Need](./LLM/Transformer/Attention-Is-All-You-Need.pdf/)
    - [BERT](./LLM/BERT/)
      - [[Code] BERT](./LLM/BERT/BERT/)
      - [[Paper] Pre-training of Deep Bidirectional Transformers for Language Understanding](./LLM/BERT/Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding.pdf)
  - [MultiModal LLM](./MultiModal-LLM/)
    - [VIT](./MultiModal-LLM/1-VIT/)
      - [[Code] VIT](./MultiModal-LLM/1-VIT/VIT/)
      - [[Paper] Vision Transformer: An Image is Worth 16x16 Words Transformers for Image Recognition at Scale](./MultiModal-LLM/1-VIT/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale.pdf)
    - [CLIP](./MultiModal-LLM/2-CLIP/)
      - [[Code] CLIP](./MultiModal-LLM/2-CLIP/CLIP/)
      - [[Paper] CLIP: Learning Transferable Visual Models From Natural Language Supervision](./MultiModal-LLM/2-CLIP/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.pdf)
    - [MoCo](./MultiModal-LLM/3-MoCo/)
      - [[Paper] MoCo: Momentum Contrast for Unsupervised Visual Representation Learning](./MultiModal-LLM/3-MoCo/Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning.pdf)
    - [ALBEF](./MultiModal-LLM/4-ALBEF/)
      - [[Paper] ALBEF: Align before Fuse](./MultiModal-LLM/4-ALBEF/Align-before-Fuse.pdf)
    - [BILP](./MultiModal-LLM/5-BILP/)
      - [[Paper] BILP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](./MultiModal-LLM/5-BILP/Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation.pdf)
    - [Flamingo](./MultiModal-LLM/6-Flamingo/)
      - [[Paper] Flamingo a Visual Language Model for Few-Shot Learning](./MultiModal-LLM/6-Flamingo/Flamingo-a-Visual-Language-Mode-for-Few-Shot-Learning.pdf)
    - [BLIP-2](./MultiModal-LLM/7-BLIP-2/)
      - [[Paper] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](./MultiModal-LLM/7-BLIP-2/Bootstrapping-Language-Image-Pre-training-with-Frozen-Image-Encoders-and-Large-Language-Models.pdf)
    - [LLaVA](./MultiModal-LLM/8-LLaVA/)
      - [[Paper] LLaVA: Visual Instruction Tuning](./MultiModal-LLM/8-LLaVA/Visual-Instruction-Tuning.pdf)
    - [LLaMA](./MultiModal-LLM/9-LLaMA/)
      - [[Paper] LLaMA: Open and Efficient Foundation Language Models](./MultiModal-LLM/9-LLaMA/Open-and-Efficient-Foundation-Language-Models.pdf)
  - [README.md](./README.md)



## üôè Acknowledgement

Special thanks to:

- **Prof. Shiyu Zhao** for his course *"Mathematical Foundations of Reinforcement Learning"* ([GitHub](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning)).
- **Autolabor** for their **ROS Tutorials** ([link](http://www.autolabor.com.cn/book/ROSTutorials/index.html)).
- The authors of **Practical PyTorch** ([GitHub](https://github.com/spro/practical-pytorch)) for their clear PyTorch examples.
- **Phil Wang (lucidrains)** for the **ViT-PyTorch** implementation ([GitHub](https://github.com/lucidrains/vit-pytorch)).

Their resources greatly supported this work.

## ‚ú® Contribution Guide

Contributions are welcome in any form, including:

- Adding more reinforcement learning materials
- Improving and optimizing example code
- Sharing learning experiences and practical tips

Please contact me via Pull Requests or Issues.

## üìú License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.

## üîó Contact

GitHub: [https://github.com/QinCheng0928](https://github.com/QinCheng0928)
